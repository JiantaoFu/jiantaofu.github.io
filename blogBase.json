{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekVercount.js'></script>", "title": "The Curious Path", "subTitle": "\u597d\u5947\u7684\u63a2\u7d22\u4e4b\u8def", "avatarUrl": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/The Pragmatic Programmer-du-hou-gan.html", "labels": ["Books"], "postTitle": "The Pragmatic Programmer\u8bfb\u540e\u611f", "postUrl": "post/The%20Pragmatic%20Programmer-du-hou-gan.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/1", "commentNum": 0, "wordCount": 11533, "description": "\u8fd9\u672c\u4e66\u7684\u526f\u6807\u9898\u662fFrom Journeyman to Master\uff0c\u4e2d\u6587\u7ffb\u8bd1\u8fc7\u6765\u662f\u201d\u4ece\u5c0f\u5de5\u5230\u4e13\u5bb6\u201d\uff0c\u5de5\u4f5c\u4e86\u8fd1\u5341\u5e74\uff0c\u56de\u8fc7\u5934\u6765\u518d\u770b\u8fd9\u672c\u4e66\uff0c\u53d1\u73b0\u8fd9\u672c\u4e66\u867d\u7136\u4ece\u51fa\u7248\u5230\u73b0\u5728\u5df2\u7ecf\u8fd120\u5e74\u4e86\uff0c\u6b63\u5982\u4f5c\u8005\u572809\u5e74\u7684\u518d\u7248\u5e8f\u8a00\u91cc\u8bf4\u7684:\u201dThings Really Haven\u2019t Changed That Much\u201d\uff0c\u53c8\u4e00\u4e2a\u5341\u5e74\u8fc7\u53bb\u4e86\uff0c\u4e66\u4e2d\u7684\u6700\u4f73\u5b9e\u8df5\u653e\u5012\u4eca\u5929\u4f9d\u7136\u9002\u7528\u3002", "top": 0, "createdAt": 1736295107, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-08", "dateLabelColor": "#0969da"}, "P2": {"htmlDir": "docs/post/ji-ta-he-xian-ji-chu-li-lun.html", "labels": ["Music"], "postTitle": "\u5409\u4ed6\u548c\u5f26\u57fa\u7840\u7406\u8bba", "postUrl": "post/ji-ta-he-xian-ji-chu-li-lun.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/2", "commentNum": 0, "wordCount": 4849, "description": "## \u97f3\u7b26\r\n\r\n\u97f3\u4e50\u670912\u4e2a\u97f3\u7b26(Note)\uff0c\u6bcf\u4e2a\u97f3\u7b26\u4e4b\u95f4\u9694\u4e00\u4e2a\u534a\u97f3\uff0c\u6bd4\u5982B\u548cC\u4e4b\u95f4\u5c31\u9694\u4e00\u4e2a\u534a\u97f3(H)\uff0cA\u548cB\u4e4b\u95f4\u9694\u4e00\u4e2a\u5168\u97f3(W, \u5373\u4e24\u4e2a\u534a\u97f3):\r\n\r\n A   |  A&#9839;/B&#9837;  |  B  |  C  |  C&#9839;/D&#9837;  |  D  | D&#9839;/E&#9837;  |  E  |  F  |  F&#9839;/G&#9837;  |  G  |  G&#9839;/A&#9837;\r\n:---:|:-------------------:|:---:|:---:|:-------------------:|:---:|:------------------:|:---:|:---:|:-------------------:|:---:|:------------------:\r\n 1   |      2              |  3  |  4  |              5      |  6  |          7         |  8  |  9  |          10         |  11 |         12\r\n \r\n \u97f3\u7b26\u5728\u5409\u4ed6\u4e0a\u7684\u4f4d\u7f6e\uff0c\u4ece\u5de6\u5230\u53f3\u5bf9\u5e940-12\u4e2a\u97f3\u683c(Fred)\uff0c\u4ece\u4e0a\u5230\u4e0b\u5bf9\u5e941-6\u5f26\uff1a\r\n \r\n0 |  1   |      2              |  3  |  4  |              5      |  6  |          7         |  8  |  9  |          10         |  11 \r\n:---:|:-------------------:|:---:|:---:|:-------------------:|:---:|:------------------:|:---:|:---:|:-------------------:|:---:|:------------------:\r\nE | F | F&#9839;/G&#9837; | G | G&#9839;/A&#9837; | A | A&#9839;/B&#9837; | B | C | C&#9839;/D&#9837; | D | D&#9839;/E&#9837; \r\nB | C | C&#9839;/D&#9837; | D | D&#9839;/E&#9837; | E | F | F&#9839;/G&#9837; | G | G&#9839;/A&#9837; | A | A&#9839;/B&#9837; \r\nG | G&#9839;/A&#9837; | A | A&#9839;/B&#9837; | B | C | C&#9839;/D&#9837; | D | D&#9839;/E&#9837; | E | F | F&#9839;/G&#9837; \r\nD | D&#9839;/E&#9837; | E | F | F&#9839;/G&#9837; | G | G&#9839;/A&#9837; | A | A&#9839;/B&#9837; | B | C | C&#9839;/D&#9837; \r\nA | A&#9839;/B&#9837;  | B | C | C&#9839;/D&#9837; | D | D&#9839;/E&#9837; | E | F | F&#9839;/G&#9837; | G | G&#9839;/A&#9837;\r\nE | F | F&#9839;/G&#9837; | G | G&#9839;/A&#9837; | A | A&#9839;/B&#9837; | B | C | C&#9839;/D&#9837; | D | D&#9839;/E&#9837; \r\n\r\n## \u97f3\u9636(Scale)\r\n\r\n\u5e38\u89c1\u7684\u6709\u5927\u8c03\u97f3\u9636\u548c\u5c0f\u8c03\u97f3\u9636\uff0c\u5b83\u4eec\u7684\u533a\u522b\u5728\u4e8e\u5168\u97f3(W)\u548c\u534a\u97f3(H)\u7684\u6392\u5217\u987a\u5e8f\u4e0d\u540c\uff0c\u800c\u8fd9\u79cd\u4e0d\u540c\u7684\u987a\u5e8f\u4f1a\u5e26\u6765\u4e0d\u540c\u7684\u98ce\u683c\u3002", "top": 0, "createdAt": 1736297528, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-08", "dateLabelColor": "#0969da"}, "P3": {"htmlDir": "docs/post/TCP Cubic Congestion Control Paper and Linux Implementation.html", "labels": ["Linux"], "postTitle": "TCP Cubic Congestion Control Paper and Linux Implementation", "postUrl": "post/TCP%20Cubic%20Congestion%20Control%20Paper%20and%20Linux%20Implementation.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/3", "commentNum": 0, "wordCount": 21379, "description": "# Paper\r\n\r\nuse a cubic function to replace the linear window growth to improve performance in high BDP network, also achieved RTT fairness.\r\n\r\ntcp-cubic is succssor of tcp-bic, tcp-bic use binary search to find the avialble bandwidth when congestion happens, and MAY enter an lieaner increasing state (Additive Increase)if it dropped too low, if it passed the maximum window, it goes into max probing state and will do the reverse and grow slowly initially and switch to additive increase if it can't find new maximum window nearby.\r\n\r\nThe problem with tcp-bic is that the growth function is too aggressive for short RTT or low speed network, and different phases (additive increase, binary search, max probing) add complexity in implememting and analyzing.\r\n\r\ntcp-cubic is replacing the phase functions with a unified one function, which is a cubic function of the elapsed time from the last congestion event.\r\n\r\nfor the tcp-cubic function, we can start with `w=t^^3`, which gives the initial shape center at (0, 0), then we can move the center towards the right to K, which gives `w = (t-K)^^3`, and then move the center upwards to Wmax, then we have `w = (t-K)^^3 + Wmax`, and we can tune the shape via parameter C, the larger C, more aggressive on the steps, now we have **equation (1)** `w = C*(t-K)^^3 + Wmax`.\r\n\r\nSay if now we have congestion and the congestion window drops from `Wmax` to `Wmax*(1 - beta)`, beta is the window decrease factor. And we assume this point is `(0, Wmax*(1 - beta)`, then K is actually the time it takes to ramp up back to Wmax. and we can get the **equation (2)**:\r\n\r\n```\r\ncwnd = Wmax - Wmax * beta = C * (0 - K)^^3 + Wmax\r\n=>  Wmax * beta = C * K^^3 \r\n=>  K = (Wmax * beta / C)^^(1/3)\r\n```\r\n\r\nThat is how we get the eqaution (1) and (2) in the paper.\r\n\r\nIn each RTT period, we calcuate curent Window size using equation (1), `W_curr = W(t + RTT)`.\r\n\r\nThe current window might be to small compared with traditional TCP, and might cause friendly issue(actually no wrose than traditional TCP in anyway), so it will give an estimation of the traditional TCP congestion window after time t, and if the window calcualated using cubic function is smaller than that, it will use the traditional TCP congestion window.\r\n\r\nTo make it fairness with traditional TCP, we should get similar average window size. \r\n\r\nThe average window size of AIMD additive increase (alpha) and multplicative decrease (beta) is give as `sqrt[alpha/2 * (2-beta)/beta / p] / RTT`, for tcp alpha = 1 and beta = 0.5, so we have average window size for traditional TCP `sqrt(3/2/p) / RTT`. Since now in use different beta (0.2) for cubic, to get similiar average window size, the alpha should be `3 * beta / (2 - beta)`, and we can caculate the window size if using AIMD with alpha and beta, given the elapsed time t since last decrease.\r\n\r\n```\r\nWtcp = Wmax * (1-beta) + alpha * t / RTT = Wmax * (1-beta) + 3 * beta / (2-beta) * t / RTT\r\n```\r\n\r\nfast convergence is used to let new flow to get its fair share more quickly, the idea is that when loss event occurrs, if the Wmax keeps dropping (less than before), it will reduce more to apply a factor `1 - beta/2` (that is * 0.9), so that the flow have the plateau earlier to give other flows more chance to catch up.\r\n\r\n# Linux implementation\r\n\r\nAt a first glance, the Linux implementation looks more complicate than the paper, the reason is that for kernel peformance, for example it convert all the float number operation into integer by scaling, and also the congestion avoidance implementation is actually increase the congestion window by 1 every N ACKs, it won't be able to increase by a float number calculated using the equation (1), we have to calculate a N to approximate that to achieve similar effect, and need to consider the effect of delay-ack. Another confusing thing is that the beta in linux implementation is actually 1-beta in the paper. The code also include the hystart slow start algorithm which is not in the cubic paper.\r\n\r\nFirst, let's see how the code use the equations above, the first thing is when there is a packet drop, it will update `ssthresh` and `last_max_cwnd`, beta is 717 and BICTCP_BETA_SCALE is 1024, beta/BICTCP_BETA_SCALE = 0.7, so the `snd_ssthresh` will drop to `0.7 * snd_cwnd`. And `snd_cwnd` will be updated based on the state of TCP (todo add more details, see tcp_input.c)\r\n\r\nIf no fast convergence, `last_max_cwnd` will set to the current `snd_cwnd`, otherwise it will apply another factor: `(BICTCP_BETA_SCALE + beta) / (2 * BICTCP_BETA_SCALE) = (1+beta_before_scaled)/2 = (1+1-beta_in_paper)/2 = (2 - beta_in_paper)/2 = (1 - beta_in_paper/2`\r\n\r\n```\r\nstatic u32 bictcp_recalc_ssthresh(struct sock *sk)\r\n{\r\n\tconst struct tcp_sock *tp = tcp_sk(sk);\r\n\tstruct bictcp *ca = inet_csk_ca(sk);\r\n\r\n\tca->epoch_start = 0;\t/* end of epoch */\r\n\r\n\t/* Wmax and fast convergence */\r\n\tif (tp->snd_cwnd < ca->last_max_cwnd && fast_convergence)\r\n\t\tca->last_max_cwnd = (tp->snd_cwnd * (BICTCP_BETA_SCALE + beta))\r\n\t\t\t/ (2 * BICTCP_BETA_SCALE);\r\n\telse\r\n\t\tca->last_max_cwnd = tp->snd_cwnd;\r\n\r\n\tca->loss_cwnd = tp->snd_cwnd;\r\n\r\n\treturn max((tp->snd_cwnd * beta) / BICTCP_BETA_SCALE, 2U);\r\n}\r\n```\r\n\r\nWhen get ack, it will calculate a moving average of delayed acked packet count `delayed_ack`, `ACK_RATIO_SHIFT`(default to 4) is the moving average factor and `cnt` is the delayed acked packet count in current ACK. `delayed_ack` will be used later to update congestion window, in the paper it assumes that we update congestion window on each ACK per RTT, and delayed ack might gives longer interval per ACK, and we need compensate for that.\r\n\r\nThe following code is actuall doing the math `ratio = (15*ratio + sample) / 16`, `delayed_ack`(ratio) is initilized to be 16 times larger `ca->delayed_ack = 2 << ACK_RATIO_SHIFT;`, if the last value is X without scaling by 16 times (`ratio = 16 * X`), then `15 * ratio / 16 + cnt = 15 * 16 * X / 16 + cnt = 15 * X + cnt`, `15 * ratio / 16 + cnt` is what used in the code below, and if we check the above equation in reverse direction: moving average of the raw value X without scaling is `(15 * X + cnt) / 16 = (15 * 16 * X / 16 + cnt) / 16 = (15 * ratio / 16 + cnt) / 16`, that is we can calculate moving average X using the scaled value ratio, and we need `* 16` to convert the raw value to `ratio` for next iteration, and store that in `delayed_ack`, so the `delayed_ack` is actually the moving avarege of scaled value.\r\n\r\n```\r\nstatic void bictcp_acked(struct sock *sk, u32 cnt, s32 rtt_us)\r\n{\r\n    ......\r\n                u32 ratio = ca->delayed_ack;\r\n\t\tratio -= ca->delayed_ack >> ACK_RATIO_SHIFT;\r\n\t\tratio += cnt;\r\n\t\tca->delayed_ack = min(ratio, ACK_RATIO_LIMIT);\r\n    ......\r\n}\r\n```\r\n\r\nIf the current `snd_cwnd` is no larger than `snd_ssthresh` it will enter into slow start, can either use the traditional slow start or hystart based on the settings, otherwise, it will enter into congestion avoidance, this is what cubic does most of its job:\r\n\r\nIf current `cwnd` is larger than `last_max_cwnd`, will reset the origin point to `cwnd`, and reset `bic_K` to zero, this is the max probing state. Else it's in steady state, and will compute new K, the estimate time needed to reach `last_max_cwnd`, we can rewrite equation 2 `cwnd = Wmax - Wmax * beta = C * (0 - K)^^3 + Wmax => K = [(Wmax - cwnd) / C]^^(1/3)`, this is the equation used in the code.\r\n\r\n```\r\n    if (ca->last_max_cwnd <= cwnd) {\r\n\t\t\tca->bic_K = 0;\r\n\t\t\tca->bic_origin_point = cwnd;\r\n\t\t} else {\r\n\t\t\t/* Compute new K based on\r\n\t\t\t * (wmax-cwnd) * (srtt>>3 / HZ) / c * 2^(3*bictcp_HZ)\r\n\t\t\t */\r\n\t\t\tca->bic_K = cubic_root(cube_factor\r\n\t\t\t\t\t       * (ca->last_max_cwnd - cwnd));\r\n\t\t\tca->bic_origin_point = ca->last_max_cwnd;\r\n\t\t}\r\n```\r\n\r\nshould be notice that the unit of bic_K (same for the elapsed time since epoch) is not ms nor jiffies(HZ), it's using `BHZ = HZ*(2^^BICTCP_HZ) = HZ*(2^^10) = HZ*1024`, to avoid overflow when doing the math.\r\n\r\nThe C used is actually `bic_scale * 10 / 1024 =  41 * 10 / 1024 =  0.4`, and in order to convert the time to units of BHZ, `bic_K (HZ) = cubic_root(1/C * (last_max_cwnd - cwnd)) => bic_K (BHz) * 2^^BICTCP_HZ = 2^^BICTCP_HZ * cubic_root(1/C * (last_max_cwnd - cwnd)) = cubic_root((2^^BICTCP_HZ)^^3 * 1/C * (last_max_cwnd - cwnd)) = cubic_root(2^^(3*BICTCP_HZ) * 1/C * (last_max_cwnd - cwnd)) = cubic_root(2^^(3*BICTCP_HZ) * 2^^10/(bic_scale*10) * (last_max_cwnd - cwnd))`\r\n\r\n`2^^(3*BICTCP_HZ) * 2^^10/(bic_scale*10) = 2^^(3*BICTCP_HZ+10)/(bic_scale*10) = 1ull << (10+3*BICTCP_HZ)/(bic_scale*10)`, this is how `cube_factor` is caculated.\r\n\r\n```\r\ncube_factor = 1ull << (10+3*BICTCP_HZ);\r\ndo_div(cube_factor, bic_scale * 10);\r\n```\r\n\r\nThen we will caculate the elapsed time plus one RTT in unit BHZ, and then caculate the target congestion window, for calculating delta of congestion window, `delta = (cube_rtt_scale * offs * offs * offs) >> (10+3*BICTCP_HZ);`, since the `offset` is in BHZ, to convert it back to HZ, we need `/ (2^^BICTCP_HZ)`, that is `>>BICTCP_HZ`, and we have multiplied 3 offset, so it's converting 'BHZ * BHZ * BHZ' back to 'HZ * HZ * HZ', that is `/(2^^BICTCP_HZ)^^3 = /(2^^(3*BICTCP_HZ) = >>(3*BICTCP_HZ)`, and `C = cube_rtt_scale/1024 = cube_rtt_scale >> 10`.\r\n\r\nso `delta = C * offs * offs * offs >> (3+BICTCP_HZ) = cube_rtt_scale * offs * offs * offs >> (10+3*BICTCP_HZ)`\r\n\r\n```\r\n\tt = ((tcp_time_stamp + msecs_to_jiffies(ca->delay_min>>3)\r\n\t      - ca->epoch_start) << BICTCP_HZ) / HZ;\r\n\r\n\tif (t < ca->bic_K)\t\t/* t - K */\r\n\t\toffs = ca->bic_K - t;\r\n\telse\r\n\t\toffs = t - ca->bic_K;\r\n\r\n\t/* c/rtt * (t-K)^3 */\r\n\tdelta = (cube_rtt_scale * offs * offs * offs) >> (10+3*BICTCP_HZ);\r\n\tif (t < ca->bic_K)                                \t/* below origin*/\r\n\t\tbic_target = ca->bic_origin_point - delta;\r\n\telse                                                \t/* above origin*/\r\n\t\tbic_target = ca->bic_origin_point + delta\r\n```\r\n\r\nOnce we have `bic_target`, we can calculate the congestion window increment for each RTT, `tcp_cong_avoid_ai` is called for each ACK, and it will increase `snd_cwnd` by 1 on every `w` ACKs (that is increase by `1/w` on each ACK), for TCP reno `w` is the congestion window, that is increment by 1 for each RTT (receive `w` ACKs when `w` packets send out in one RTT).\r\n\r\n```\r\n/* In theory this is tp->snd_cwnd += 1 / tp->snd_cwnd (or alternative w) */\r\nvoid tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w)\r\n{\r\n\tif (tp->snd_cwnd_cnt >= w) {\r\n\t\tif (tp->snd_cwnd < tp->snd_cwnd_clamp)\r\n\t\t\ttp->snd_cwnd++;\r\n\t\ttp->snd_cwnd_cnt = 0;\r\n\t} else {\r\n\t\ttp->snd_cwnd_cnt++;\r\n\t}\r\n}\r\n```\r\n\r\nFor cubic, we are expecting to increase the congestion window by `bic_target - cwnd` in the next round trip time, since we will send out `cwnd` packets in one RTT, if we don't consider delayed ack and assume ack per packet, then it's for each ACK the congestion window increases by `(bic_target - cwnd)/cwnd`, then `(bic_target - cwnd)/cwnd = 1/w => w = cwnd/ (bic_target - cwnd)`, which is exactly what we have for `ca->cnt` when `bic_target > cwnd`, if it's smaller, then the increment will be very small, `1/(100 * cwnd)` on each ACK, that is `1/100` on each RTT if no delay ACK, this is just a safety guard.\r\n\r\n```\r\n\t/* cubic function - calc bictcp_cnt*/\r\n\tif (bic_target > cwnd) {\r\n\t\tca->cnt = cwnd / (bic_target - cwnd);\r\n\t} else {\r\n\t\tca->cnt = 100 * cwnd;              /* very small increment*/\r\n\t}\r\n\t\r\n\t...\r\n\ttcp_cong_avoid_ai(tp, ca->cnt);\r\n```\r\n\r\nAnd if we consider the delay ack, we should compensate for that b/c we may get less Ack when using delay ack, which means for each ACK we should increase more, that said if no delay ACK, we increase `1/ca->cnt` per ACK (w = ca->cnt), then with delay ACK, assume we have pkts_per_ack, we should increase `pkts_per_ack/ca->cnt` per ACK (w = ca->cnt/pktsPerAck . `delayed_ack` is a scaled value, then `pkts_per_ack = delayed_ack/16`\r\n\r\n```\r\nca->cnt = (ca->cnt << ACK_RATIO_SHIFT) / ca->delayed_ack;\r\n```\r\n\r\nTo avoid the case cubic is slower than TCP, it will estimated the congestion window of traditional TCP. In the paper we have the increament factor `alpha = 3 * beta_in_paper / (2-beta_in_paper)`, since in the code the beta is actual `1-beta_in_paper`, then `alpha = 3 * (1 - beta) / (1 + beta)`.\r\n\r\nin the code, `beta_scale = 8 / alpha`, 8 is just another scaling factor which ensure beta_scale is integer, and for later use it will apply `>>3` which give `1 / alpha`.\r\n\r\n```\r\nbeta_scale = 8*(BICTCP_BETA_SCALE+beta)/ 3 / (BICTCP_BETA_SCALE - beta);\r\n```\r\n\r\nWhen `tcp_friendliness` enabled, to simulate traditional TCP, the congestion window increase per RTT is `alpha`, so for each ACK, the increment `1/w` is `alpha/cwnd = 8/(beta_scale * cwnd)`, so we have `w = beta_scale * cwnd / 8 = beta_scale * cwnd >> 3`. That is every `w` ACK packets the congestion window increment by 1, give current has `ca->ack_cnt` acks in total, we can estimate `tcp_cwnd`.\r\n\r\nonce we have `tcp_cwnd`, if it's larger than current `cwnd`, we should increase cwnd by `delta = tcp_cwnd - cwnd`, and we use the max between `bic_target` and `tcp_cwnd`, the code is using smaller cnt but it's actually the same.\r\n\r\n```\r\n\t/* TCP Friendly */\r\n\tif (tcp_friendliness) {\r\n\t\tu32 scale = beta_scale;\r\n\t\tdelta = (cwnd * scale) >> 3;\r\n\t\twhile (ca->ack_cnt > delta) {\t\t/* update tcp cwnd */\r\n\t\t\tca->ack_cnt -= delta;\r\n\t\t\tca->tcp_cwnd++;\r\n\t\t}\r\n\r\n\t\tif (ca->tcp_cwnd > cwnd){\t/* if bic is slower than tcp */\r\n\t\t\tdelta = ca->tcp_cwnd - cwnd;\r\n\t\t\tmax_cnt = cwnd / delta;\r\n\t\t\tif (ca->cnt > max_cnt)\r\n\t\t\t\tca->cnt = max_cnt;\r\n\t\t}\r\n\t}\r\n```\r\n\r\n# Hystart\r\n\r\nLinux implementation also use hystart slow start by default, hystart slow start is to exit early to avoid too many packet loss caused by slow start phase, and it utilize the packet train and delay to give hint when to stop slow start. Since for most TCP it's window based and data are send out in a burst in one congestion window, which means we can use those packets as packet train, suppose we send out N packets in the train, the time gap between the 1st and Nst packet is delta(N), then the bandwidth estimation will be `bw = (N-1) * packet_length / delta(N)`,  the network pipe capcaity (without buffer) is `K = bw * one_way_delay_min`, the data sent in a cwnd should be no larger than the pipe capacity.\r\n\r\n```\r\n(N-1) * packet_length <= bw * one_way_delay_min = (N-1) * packet_length / delta(N) * one_way_delay_min\r\n=> delta(N) <= one_way_delay_min\r\n```\r\n\r\nThat means we can measure the time gap of the train to know whether we are exceed the available bandwidth and enter into congestion avoidance. Since it's not easy to measure one way delay, half of RTT is used, and to avoid modification on both sides, ACK gap between Nst and 1st packet is used. \r\n\r\nUsing RTT/2 as one way delay estimation won't make things worse,  exit too early will cause under untilization, `beta * (bw * one_way_delay_min)` can be used as the lowerbound for safety exit, for standard TCP, beta is 0.5, other variances has beta larger than 0.5;\r\n\r\nsuppose we have forward and backward delay `a` and `b`, if we use `RTT/2 = (a+b)/2` as one way delay, the BDP estimation `K' = bw * (a+b)/2` while `K =  bw * a`, `K'/K = (a + b) / 2 / a = 1/2 + b/2/a >= 1/2`, so we have `K' >= 0.5 * K`, that is to say if we use RTT/2 as estimation, the BDP estimations is no less than half of the real BDP, so we won't exit slow start before we reach 0.5 * BDP (threshold of the standard TCP slow start).  \r\n\r\nAnd if it congestin window goes beyond `K + S`(S is network buffer size), which is the upper bound of safety exit, packet will be dropped, so we have `K'/K = 1/2 + b/2/a <= (K + S)/K => 1/2 + b/2/a <= 1 + S/K`, if `S=K`, then `1/2 + b/2/a <= 2 => b/a <= 3 => b <= 3a`, and there are less than 5% cases that has reverse path delay larger than forward path, which means in this case, the slow start will fallback to traditional slow start which overshoots and cause packet loss.\r\n\r\nUsing Ack may give larger time gap, which gives lower bandwidth estimation, and cause conservative behavior to exit slow start earlier. Delay Ack also affect the accuracy, so if there is significantly delay in the last ACK, that sample is filter out and mixed with next train.\r\n\r\nAnd there may be cases that minimum RTT is not available, for example when multiple flows are competing, the idea is use delay increasing as an exit indicator, it measure the first a few packets of the train, and calculate the average RTT for that train, and compare the trian K and train K-1, if RTT(K) > RTT(K-1) + delta, then exit slow start.\r\n\r\nThe Linux implementation is slightly different and simpler than that in the paper.\r\n\r\nHystart will be triggered when the cwnd is larger than `hystart_low_window` (default to 16)\r\n\r\n```\r\n\t/* hystart triggers when cwnd is larger than some threshold */\r\n\tif (hystart && tp->snd_cwnd <= tp->snd_ssthresh &&\r\n\t    tp->snd_cwnd >= hystart_low_window)\r\n\t\thystart_update(sk, delay);\r\n```\r\n\r\nIt will keep track to minimum delay(`delay_min`) in the tcp session so far. On each ACK, it will check if the time since last ACK is less than the threshold `hystart_ack_delta` to filter the invalid sample, if it's invalid, then in this round, it will not do ack train detection, and if the sample is valid, check if it goes beyond minRTT/2 (`delay_min` is scaled by 8, so `>>4` is actually minRTT/2), if so it will detect as `HYSTART_ACK_TRAIN` happens.\r\n\r\nIt also track minimum delay(`curr_delay`) among the first `HYSTART_MIN_SAMPLES` samples in each round (send cwnd packets in the burst train). It `curr_delay` is larger than `delay_min` more than a threshold value, which is `delay_min/2`, clamp to `[4, 16] ms`, then it will detect as `HYSTART_DELAY`.\r\n\r\nEither one detected will cause the slow start exit by setting the `ssthresh` to current congeston window `snd_cwnd`.\r\n\r\n```\r\nstatic void hystart_update(struct sock *sk, u32 delay)\r\n{\r\n\tstruct tcp_sock *tp = tcp_sk(sk);\r\n\tstruct bictcp *ca = inet_csk_ca(sk);\r\n\r\n\tif (!(ca->found & hystart_detect)) {\r\n\t\tu32 now = bictcp_clock();\r\n\r\n\t\t/* first detection parameter - ack-train detection */\r\n\t\tif ((s32)(now - ca->last_ack) <= hystart_ack_delta) {\r\n\t\t\tca->last_ack = now;\r\n\t\t\tif ((s32)(now - ca->round_start) > ca->delay_min >> 4)\r\n\t\t\t\tca->found |= HYSTART_ACK_TRAIN;\r\n\t\t}\r\n\r\n\t\t/* obtain the minimum delay of more than sampling packets */\r\n\t\tif (ca->sample_cnt < HYSTART_MIN_SAMPLES) {\r\n\t\t\tif (ca->curr_rtt == 0 || ca->curr_rtt > delay)\r\n\t\t\t\tca->curr_rtt = delay;\r\n\r\n\t\t\tca->sample_cnt++;\r\n\t\t} else {\r\n\t\t\tif (ca->curr_rtt > ca->delay_min +\r\n\t\t\t    HYSTART_DELAY_THRESH(ca->delay_min>>4))\r\n\t\t\t\tca->found |= HYSTART_DELAY;\r\n\t\t}\r\n\t\t/*\r\n\t\t * Either one of two conditions are met,\r\n\t\t * we exit from slow start immediately.\r\n\t\t */\r\n\t\tif (ca->found & hystart_detect)\r\n\t\t\ttp->snd_ssthresh = tp->snd_cwnd;\r\n\t}\r\n}\r\n```\r\n\r\n# Tuning parameters\r\n\r\nLinux cubic implementation has some parameters which can be used to tune the algorithm, by set different values for files (filename is the same with parameter name) in `/sys/module/tcp_cubic/parameters/`\r\n\r\n- fast_convergence: default is on. enable fast convergence will degrade more when consecutive downgrade, this is used for let new comers to get fair share faster. \r\n\r\n- beta: beta is used for multiplicative decrease, larger value will decrease less, default is 0.7, that is 0.7 * cwnd when loss happens. beta will also affect the fairness with standard TCP. beta is scaled by 1024.\r\n\r\n- initial_ssthresh: use to set the initial ssthresh value, only used at the beginning whe hystart not enabled.\r\n\r\n- bic_scale: bi_scale is used to tune the cubic funtion curve (C in the equation 1), large beta will increase more aggressively, default is 0.4. bic_scale also affect teh fairness with standard TCP. bic_scale is scaled by 1024.\r\n\r\n- tcp_friendliness: used to ensure in any case cubic is no worse than standard TCP. default is on.\r\n\r\n- hystart: enable hystart, which exit slow start earlier based on delay to avoid too much packet loss.\r\n\r\n- hystart_detect: enable which methods are used for hystart detection, default is enable both packet train and delay increase.\r\n\r\n- hystart_low_window: when cwnd is larger than hystart_low_window, will start hystart slow start. default is 16.\r\n\r\n- hystart_ack_delta: threshold value to filter delay ACK, default is 2.\r\n\r\n```\r\nmodule_param(fast_convergence, int, 0644);\r\nMODULE_PARM_DESC(fast_convergence, 'turn on/off fast convergence');\r\nmodule_param(beta, int, 0644);\r\nMODULE_PARM_DESC(beta, 'beta for multiplicative increase');\r\nmodule_param(initial_ssthresh, int, 0644);\r\nMODULE_PARM_DESC(initial_ssthresh, 'initial value of slow start threshold');\r\nmodule_param(bic_scale, int, 0444);\r\nMODULE_PARM_DESC(bic_scale, 'scale (scaled by 1024) value for bic function (bic_scale/1024)');\r\nmodule_param(tcp_friendliness, int, 0644);\r\nMODULE_PARM_DESC(tcp_friendliness, 'turn on/off tcp friendliness');\r\nmodule_param(hystart, int, 0644);\r\nMODULE_PARM_DESC(hystart, 'turn on/off hybrid slow start algorithm');\r\nmodule_param(hystart_detect, int, 0644);\r\nMODULE_PARM_DESC(hystart_detect, 'hyrbrid slow start detection mechanisms'\r\n\t\t ' 1: packet-train 2: delay 3: both packet-train and delay');\r\nmodule_param(hystart_low_window, int, 0644);\r\nMODULE_PARM_DESC(hystart_low_window, 'lower bound cwnd for hybrid slow start');\r\nmodule_param(hystart_ack_delta, int, 0644);\r\nMODULE_PARM_DESC(hystart_ack_delta, 'spacing between ack's indicating train (msecs)');\r\n```\u3002", "top": 0, "createdAt": 1736314494, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-08", "dateLabelColor": "#0969da"}, "P4": {"htmlDir": "docs/post/Piem- Network Emulator based on Raspberry Pi.html", "labels": ["Linux"], "postTitle": "Piem: Network Emulator based on Raspberry Pi", "postUrl": "post/Piem-%20Network%20Emulator%20based%20on%20Raspberry%20Pi.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/4", "commentNum": 0, "wordCount": 4182, "description": "# Basic setup\r\n\r\nYou can use [Raspberry Pi 4](https://www.amazon.com/GeeekPi-Raspberry-4GB-Starter-Kit/dp/B0B3M2HKN6/ref=sr_1_12_sspa?crid=2L2JEUJ1460WB&dib=eyJ2IjoiMSJ9.dm2FB5zZz0j71xQemzz3eGcfMj6yAgNj0EPv-ZREy2PWNx49GJkdHYAzErmjO1rWARxfo12XUNdoTscBLFfbLbRIYyt2O44eh6kBKePKgZHoWzqG1x61SEy_n7vicrrWGgnVkLoWBoS1rY0LU453Xc8SXZtyYgzT9ZGvLWKovR1k4qtC_FNH2zz6wdvKDwm1cEi8riFowfxHNBwFO9r6uRsuyK5wCfuxhzNEeM3-ugw.eOUzPFP7-jr6HburUXKHBVMMswaal6qpIhVdoWuOVjI&dib_tag=se&keywords=raspberry+pi+starter+kit&qid=1726551003&sprefix=raspberry+pi+starter+ki%2Caps%2C176&sr=8-12-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9tdGY&psc=1) (or Raspberry Pi 3B) as a network emulator. Raspberry Pi can be used to set up a bridged network, either using ethernet or WiFi.\r\n\r\nYou can [download](https://www.dropbox.com/scl/fi/e02utgcmfw5opfymdq9h7/image_2023-05-10-piem-lite.zip?rlkey=l8b32tgl41zoqa6nupxq2y41r&st=ad66ldbu&dl=0) the prebuild image, and then download [Etcher](https://etcher.io/) and install the image on the micro sdcard.\r\n\r\nOnce done, connect keyboard monitor, and ethernet, boot and login with username \u201cpi\u201d and password \u201craspberry\u201d, then execute the following command to enable WiFi network:\r\n\r\n```\r\nrfkill unblock all\r\n```\r\n\r\nThen reboot, login again and make sure the network is working:\r\n\r\nusing ping command to make sure the internet access is working\r\n\r\nUse cell phone to scan the wifi network and find ssid \u201cpiem\u201d, and login with password \u201cpiemulator\u201d, and make sure the internet access is working\r\n\r\nIf not working, try run `sudo systemctl restart networking.service` and wait a few minutes\r\n\r\nThen you can test network impairment:\r\n\r\nRun speed test on Raspberry Pi\r\n\r\n```\r\nsudo apt install speedtest-cli\r\nspeedtest-cli\r\nRetrieving speedtest.net configuration...\r\nTesting from Comcast Cable (98.37.129.143)...\r\nRetrieving speedtest.net server list...\r\nSelecting best server based on ping...\r\nHosted by Jefferson Union High School District (Daly City, CA) [62.00 km]: 16.523 ms\r\nTesting download speed................................................................................\r\nDownload: 483.86 Mbit/s\r\nTesting upload speed......................................................................................................\r\nUpload: 112.79 Mbit/s\r\n```\r\n\r\n2. Run speed test on cell phone connected with WiFi \u201cpiem\u201d. (The speed test may not be as good as you run it on Raspberry Pi, most likely due to interference, default the channel is set to 1)\r\n\r\n3. Test rate limit, set downlink bandwidth to 3mbps, \u201c192.168.1.185\u201d is the IP address of the cellphone\r\n\r\n```\r\nsudo emulator.py add -b 3000 -f 192.168.1.185 -c downlink\r\n```\r\n\r\n4. Then run speed test on cell phone again to validate the settings\r\n\r\n5. Remove the rate limit and test it again\r\n\r\n```\r\nsudo emulator.py remove -f 192.168.1.185 -c downlink\r\n```\r\n\r\n6. Run sudo emulator.py -h for help\r\n\r\nYou can refer https://github.com/cellsim/piem for more details\r\n\r\n# Performance tune\r\n\r\nModify \u201c/etc/hostapd/hostapd.conf\u201d, change/add the following settings:\r\n\r\n1. Use 5GHz network\r\n2. Search for the channel with the least interferences\r\n3. Enable 802.11n support\r\n4. Enable 802.11ac support\r\n5. Enable QoS support\r\n6. Enable HT40 with short guard intervals\r\n\r\n```\r\nhw_mode=a\r\nchannel=0\r\nieee80211n=1\r\nieee80211ac=1\r\nwmm_enabled=1\r\nht_capab=[HT40+][SHORT-GI-40]\r\n```\r\n\r\nThen restart hostapd:\r\n\r\n```\r\nsudo systemctl restart networking.service\r\n```\r\n\r\nThere are chances that channel = 0 doesn't work (at least for me), and you won't be able to find 'piem' network anymore. You can enable logging by adding the following line in '/etc/default/hostapd':\r\n\r\n```\r\nDAEMON_OPTS='-dd -t -f /tmp/hostapd.log'\r\n```\r\n\r\nRestart hostapd and make sure it runs successfully by checking systemctl status hostapd.service, otherwise check error log.\r\n\r\nIf channel auto selection doesn\u2019t work, change that to channel = 36 or other Non-DFS Channels like: 40, 44, 48, 149, 153, 157, 161, 165. Use the following command on Raspberry Pi to check the channel in use nearby:\r\n\r\n```\r\nsudo iwlist wlan0 scan | grep 'Channel'\r\n```\r\n\r\nAfter making those changes, you should be able to see the performance boost (my test shows 3x improvement, around 60mbps ~ 80mbps).\u3002", "top": 0, "createdAt": 1736372025, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-09", "dateLabelColor": "#0969da"}, "P5": {"htmlDir": "docs/post/BigBlueButton-de-kai-yuan-shang-ye-mo-shi.html", "labels": ["Business", "Open Source"], "postTitle": "BigBlueButton\u7684\u5f00\u6e90\u5546\u4e1a\u6a21\u5f0f", "postUrl": "post/BigBlueButton-de-kai-yuan-shang-ye-mo-shi.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/5", "commentNum": 0, "wordCount": 2764, "description": "Bigbluebutton CEO Fred Dixon\u66fe\u8bf4\u8fc7\uff1a\u201d\u521b\u5efa\u4e00\u4e2a\u6210\u529f\u7684\u516c\u53f8\u4e0d\u5bb9\u6613\uff0c\u4f46\u540c\u65f6\u521b\u5efa\u4e00\u4e2a\u6210\u529f\u7684\u516c\u53f8\uff0c\u6210\u529f\u7684\u5f00\u6e90\u9879\u76ee\u548c\u6210\u529f\u7684\u751f\u6001\u7cfb\u7edf\u5c31\u662f\u96be\u4e0a\u52a0\u96be\u4e86\u3002", "top": 0, "createdAt": 1736372264, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-09", "dateLabelColor": "#0969da"}, "P6": {"htmlDir": "docs/post/wei-dai-ma-shi-ti-shi-\uff1a-rang-AI-gao-xiao-ting-dong-ni-de-xu-qiu-\uff01.html", "labels": ["AI", "Prompt Engineering"], "postTitle": "\u4f2a\u4ee3\u7801\u5f0f\u63d0\u793a\uff1a\u8ba9AI\u9ad8\u6548\u542c\u61c2\u4f60\u7684\u9700\u6c42\uff01", "postUrl": "post/wei-dai-ma-shi-ti-shi-%EF%BC%9A-rang-AI-gao-xiao-ting-dong-ni-de-xu-qiu-%EF%BC%81.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/6", "commentNum": 0, "wordCount": 3120, "description": "# \u4ec0\u4e48\u662f\u4f2a\u4ee3\u7801\u5f0f\u63d0\u793a\uff1f\r\n\r\n\u4e0d\u662f\u5199\u4ee3\u7801\uff01\u800c\u662f\u7528\u903b\u8f91\u6e05\u6670\u3001\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u544a\u8bc9AI\u4f60\u60f3\u8981\u4ec0\u4e48\uff0c\u8ba9AI\u66f4\u61c2\u4f60\uff0c\u5c11\u8e29\u5751\uff0c\u591a\u7701\u5fc3\uff01\r\n\r\n# \u4e3a\u4ec0\u4e48\u8981\u7528\u4f2a\u4ee3\u7801\u5f0f\u63d0\u793a\uff1f\r\n\r\n1\ufe0f\u20e3 \u903b\u8f91\u6e05\u6670\uff1a\u660e\u786e\u6bcf\u4e00\u6b65\u8981\u5e72\u5565\uff0cAI\u6309\u6d41\u7a0b\u6765\uff0c\u7ed3\u679c\u4e0d\u4e71\u5957\uff01\r\n\r\n2\ufe0f\u20e3 \u5185\u5bb9\u5b8c\u6574\uff1a\u4e0d\u4f1a\u5fd8\u4e1c\u5fd8\u897f\uff0c\u7ec6\u8282\u90fd\u80fd\u7167\u987e\u5230\u3002", "top": 0, "createdAt": 1736372788, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-09", "dateLabelColor": "#0969da"}, "P7": {"htmlDir": "docs/post/OpenAI coding question- OpenSheet.html", "labels": ["Programming"], "postTitle": "OpenAI coding question: OpenSheet", "postUrl": "post/OpenAI%20coding%20question-%20OpenSheet.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/7", "commentNum": 0, "wordCount": 8464, "description": "# Question\r\n\r\nToday we\u2019re going to build a basic spreadsheet application like Google sheets or Excel but much simpler. Our spreadsheet, let\u2019s call it OpenSheet, will only support cells which hold either integers or formulas that sum two cells.\r\n\r\nYou are tasked with writing a program that handles this functionality for OpenSheet. You can make any decisions you want regarding how this program is organized, but there must be some sort of setter/getter methods that can be called by the application for any given cell. All inputs will be strings.\r\n\r\nFor setting you can expect two inputs: the cell location and the cell value.\r\n\r\nExample of how your setter could look\r\n\r\n```\r\nset_cell('C1', '45')\r\nset_cell('B1', '10')\r\nset_cell('A1', '=C1+B1')\r\n```\r\n\r\nFor getting you will be provided one input that is the cell location.\r\n\r\nExample of how your getter could look\r\n\r\n```\r\nget_cell('A1') # should return 55 in this case\r\n```\r\n\r\nAssumptions:\r\n\r\n- In memory storage\r\n- All cell location inputs will be well formed (no need to validate in code)\r\n- All cell value inputs will be well formed (no need to validate in code)\r\n- Cells value inputs are either a summation of two other cells or an int\r\n- Empty cells are treated as zero when accessed\r\n\r\n# Solutions\r\n\r\nOne of the solutions found on the internet is as below, so basically\r\n\r\n1. store different types of cells separately\r\n2. replace the formula cell with valued cell for evaluation when getting the cell\r\n\r\n```python\r\nimport re\r\n\r\nclass Spreadsheet:\r\n\r\n    def __init__(self):\r\n        self.cells = {}\r\n        self.formula_cells = {}\r\n\r\n    def set_cell(self, cell, value):\r\n        self._clear_all(cell)\r\n        if value.startswith('='):\r\n            self.formula_cells[cell] = value\r\n        else:\r\n            self.cells[cell] = value\r\n\r\n    def _evaluate_formula(self, value):\r\n        formula = value\r\n        for cell in re.findall(r'[A-Z][0-9]+', value):\r\n            formula = formula.replace(cell, self.cells[cell])\r\n        return eval(formula[1:])\r\n\r\n    def get_cell(self, cell):\r\n        if cell in self.cells:\r\n            return int(self.cells[cell])\r\n        elif cell in self.formula_cells:\r\n            value = self.formula_cells[cell]\r\n            return self._evaluate_formula(value)\r\n        return 0\r\n\r\n    def _clear_all(self, cell):\r\n        if cell in self.cells:\r\n            self.cells.pop(cell)\r\n        if cell in self.formula_cells:\r\n            self.formula_cells.pop(cell)\r\n\r\nss = Spreadsheet()\r\n\r\nss.set_cell('A1', '13')\r\nss.set_cell('A2', '14')\r\nassert ss.get_cell('A1') == 13\r\nss.set_cell('A3', '=A1+A2')\r\nassert ss.get_cell('A3') == 27\r\n```\r\n\r\n> [!WARNING]\r\n> The problem with this solution is that it doesn\u2019t consider the case that the items in formula cell could be another formula cell, like:\r\n\r\n```python\r\nss.set_cell('A1', '13')\r\nss.set_cell('A2', '14')\r\nss.set_cell('A3', '=A1+A2')\r\n\r\n# will raise KeyError: 'A3'\r\nss.set_cell('A4', '=A3+A1') \r\n```\r\n\r\nAnd the fix is to call \u201c_evaluate_formula\u201d recursively\r\n\r\n```python\r\n    def _evaluate_formula(self, value):\r\n        formula = value\r\n        for cell in re.findall(r'[A-Z][0-9]+', value):\r\n            cell_value = 0\r\n            if cell in self.cells:\r\n                cell_value = self.cells[cell]\r\n            elif cell in self.formula_cells:\r\n                cell_value = self._evaluate_formula(self.formula_cells[cell])\r\n            formula = formula.replace(cell, str(cell_value))\r\n        return eval(formula[1:])\r\n```\r\n\r\n> [!WARNING]\r\n> Another corner case is \u201ccircular dependency\u201d\r\n\r\n```python\r\nss.set_cell('A1', '13')\r\nss.set_cell('A2', '14')\r\nss.set_cell('A3', '=A1+A2')\r\n\r\n# will raise RecursionError\r\nss.set_cell('B1', '=A1+C1')\r\nss.set_cell('C1', '=B1+A2')\r\nss.set_cell('B2', '=B2+A3')\r\n```\r\n\r\nTo fix this, record the visited cell when evaluating each time and raise an error if \u201ccircular dependency\u201d is found:\r\n\r\n```python\r\n    def __init__(self):\r\n        ...\r\n        self.visited = set()\r\n\r\n    def _evaluate_formula(self, value):\r\n        formula = value\r\n        for cell in re.findall(r'[A-Z][0-9]+', value):\r\n            cell_value = 0\r\n            if cell in self.cells:\r\n                cell_value = self.cells[cell]\r\n            elif cell in self.formula_cells:\r\n                if cell in self.visited:\r\n                    raise ValueError('Circular Dependency!')\r\n                self.visited.add(cell)\r\n                cell_value = self._evaluate_formula(self.formula_cells[cell])\r\n            formula = formula.replace(cell, str(cell_value))\r\n        return eval(formula[1:])\r\n\r\n    def get_cell(self, cell):\r\n        self.visited.clear()\r\n        if cell in self.cells:\r\n            return int(self.cells[cell])\r\n        elif cell in self.formula_cells:\r\n            value = self.formula_cells[cell]\r\n            return self._evaluate_formula(value)\r\n        return 0\r\n```\r\n\r\n> [!TIP]\r\n> Another enhancement could be done to improve performance is by catching the value instead of recalculated every time. This adds some complexity b/c once one cell change, all the value of cells depends on it will change as well, and the cached value will be invalid anymore, to achieve this we need to maintain the dependencies, given a cell knows which cells deps on it.\r\n\r\nHere is another solution, deps is used to maintain the cell sets that rely on specific cell, so that once we update this specific cell, we can clear the cache for those cells rely on it. Another thing need to notice is that when removing the cache, there could be chance that it has \u201ccircular dependency\u201d as well, so should check that too.\r\n\r\n```python\r\nfrom collections import defaultdict\r\n\r\nclass Cell:\r\n\r\n    def __init__(self, key, expr):\r\n        self.key = key\r\n        self.value = self.left_key = self.right_key = None\r\n        self._parse(expr)\r\n\r\n    def _parse(self, expr):\r\n        if expr.isdigit():\r\n            self.value = int(expr)\r\n        else:\r\n            self.left_key, self.right_key = expr[1:].split('+')\r\n\r\nclass Spreadsheet:\r\n\r\n    def __init__(self):\r\n        self.key_to_cell = {}\r\n        self.visited = set()\r\n        self.cache = {}\r\n        self.deps = defaultdict(set)\r\n\r\n    def set_cell(self, key, expr):\r\n        if key in self.key_to_cell:\r\n            self._clean_cache(key)\r\n            self._remove_deps(self.key_to_cell[key])\r\n            self.key_to_cell.pop(key)\r\n\r\n        cell = Cell(key, expr)\r\n        self._add_deps(cell)\r\n        self.key_to_cell[key] = cell\r\n\r\n    def _add_deps(self, cell):\r\n        if cell.value is None:\r\n            self.deps[cell.left_key].add(cell.key)\r\n            self.deps[cell.right_key].add(cell.key)\r\n\r\n    def _remove_deps(self, cell):\r\n        if cell.value is None:\r\n            self.deps[cell.left_key].remove(cell.key)\r\n            self.deps[cell.right_key].remove(cell.key)\r\n\r\n    def _clean_cache(self, key):\r\n        self.visited.clear()\r\n        self._do_clean_cache(key)\r\n        \r\n    def _do_clean_cache(self, key):\r\n        if key in self.cache:\r\n            self.cache.pop(key)\r\n            \r\n        if key not in self.deps: return\r\n        if key in self.visited: return\r\n\r\n        for k in self.deps[key]:\r\n            self.visited.add(k)\r\n            self._do_clean_cache(k)\r\n\r\n    def get_cell(self, key):\r\n         self.visited.clear()\r\n         try:\r\n            return self._evaluate(key)\r\n         except ValueError as e:\r\n            print(f'error fetching value for {key}: {e}')\r\n            return 0\r\n\r\n    def _evaluate(self, key):\r\n        if key not in self.key_to_cell:\r\n            return 0\r\n        \r\n        if key in self.cache:\r\n            return self.cache[key]\r\n        \r\n        cell = self.key_to_cell[key]\r\n        if cell.value is not None:\r\n            return cell.value\r\n        \r\n        if cell.key in self.visited:\r\n            raise ValueError('Circular Dependencies!')\r\n            return 0\r\n\r\n        self.visited.add(cell.key)\r\n        res = self._evaluate(cell.left_key) + self._evaluate(cell.right_key)\r\n        self.cache[key] = res\r\n        return res\r\n\r\nss = Spreadsheet()\r\n\r\nss.set_cell('A1', '13')\r\nss.set_cell('A2', '14')\r\nassert ss.get_cell('A1') == 13\r\nss.set_cell('A3', '=A1+A2')\r\nassert ss.get_cell('A3') == 27\r\n\r\nss.set_cell('A4', '=A3+A1')\r\nassert ss.get_cell('A4') == 40\r\n\r\nss.set_cell('B1', '=A1+C1')\r\nss.set_cell('C1', '=B1+A2')\r\nss.get_cell('B1')\r\n\r\nss.set_cell('A4', '=A2+A1')\r\nassert ss.get_cell('A4') == 27\r\n\r\nss.set_cell('A2', '15')\r\nassert ss.get_cell('A4') == 28\r\n```\u3002", "top": 0, "createdAt": 1736374596, "style": "<style>.markdown-alert{padding:0.5rem 1rem;margin-bottom:1rem;border-left:.25em solid var(--borderColor-default,var(--color-border-default));}.markdown-alert .markdown-alert-title {display:flex;font-weight:var(--base-text-weight-medium,500);align-items:center;line-height:1;}.markdown-alert>:first-child {margin-top:0;}.markdown-alert>:last-child {margin-bottom:0;}</style><style>.markdown-alert.markdown-alert-tip {border-left-color:var(--borderColor-success-emphasis, var(--color-success-emphasis));background-color:var(--color-success-subtle);}.markdown-alert.markdown-alert-tip .markdown-alert-title {color: var(--fgColor-success,var(--color-success-fg));}</style><style>.markdown-alert.markdown-alert-warning {border-left-color:var(--borderColor-attention-emphasis, var(--color-attention-emphasis));background-color:var(--color-attention-subtle);}.markdown-alert.markdown-alert-warning .markdown-alert-title {color: var(--fgColor-attention,var(--color-attention-fg));}</style>", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-09", "dateLabelColor": "#0969da"}, "P8": {"htmlDir": "docs/post/Mr. Resume- AI Resume writer.html", "labels": ["Business", "AI", "Prompt Engineering"], "postTitle": "Mr. Resume: AI Resume writer", "postUrl": "post/Mr.%20Resume-%20AI%20Resume%20writer.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/8", "commentNum": 0, "wordCount": 2011, "description": "An prompt that helps on resume writing, thanks [Mr.-Ranedeer-AI-Tutor](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor), the prompt is available at https://github.com/MrResume/ResumeBoost/.\r\n\r\nThe basic idea is to customize your resume according to a specific job description assisted with an AI resume writer, built this last year and made a landing page for that, there could be some potential to build a business around this idea.\r\n\r\nJust watched two YouTube videos related to this, very inspiring:\r\n\r\n`Gmeek-html<iframe width='560' height='315' src='https://www.youtube.com/embed/n5Th-dOI3rI?si=zwi7ftcYA7f26juS' title='YouTube video player' frameborder='0' allow='accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share' referrerpolicy='strict-origin-when-cross-origin' allowfullscreen></iframe>`\r\n\r\n\r\n`Gmeek-html<iframe width='560' height='315' src='https://www.youtube.com/embed/G4nsGvL4Fo0?si=r3mQXGNEnDIcILSx' title='YouTube video player' frameborder='0' allow='accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share' referrerpolicy='strict-origin-when-cross-origin' allowfullscreen></iframe>`\r\n\r\n# ResumeBoost\r\n\r\nResume Boost with ChatGPT / Claude\r\n\r\n# Usage\r\n\r\nCopy paste [ResumeBoost.txt](ResumeBoost.txt) into https://chat.openai.com or https://claude.ai.\r\n\r\n```\r\n/config: Configure your resume preferences, such as experience level, industry, tone style, resume length, and language.\r\n\r\n/start: Begin the process of collecting information for your resume.\r\n\r\n/done: Generate your resume based on the information you've provided.\r\n\r\n/analyse: Analyze your resume and rate it based on a job description.\r\n\r\n/continue: If you have more work experiences, education, certifications, or languages to add after generating your resume.\r\n\r\n/language [lang]: Change the language of the assistant to the specified language.\r\n\r\n/example: Get an example of how your resume might look for a specific job description.\r\n```\u3002", "top": 0, "createdAt": 1736377761, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-09", "dateLabelColor": "#0969da"}, "P9": {"htmlDir": "docs/post/WebRTC-he-da-yu-yan-mo-xing.html", "labels": ["AI"], "postTitle": "WebRTC\u548c\u5927\u8bed\u8a00\u6a21\u578b", "postUrl": "post/WebRTC-he-da-yu-yan-mo-xing.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/9", "commentNum": 0, "wordCount": 1256, "description": "\u6700\u8fd1\u770b\u5230\u4e00\u5bb6startup\u505a\u7684\u5de5\u4f5c\uff0c\u5c06WebRTC\u548c\u5927\u8bed\u8a00\u591a\u6a21\u6001\u7ed3\u5408\u8d77\u6765\uff0c\u505a\u7684\u4e00\u5957\u5b9e\u65f6\u97f3\u89c6\u9891\u7684SDK\u3002", "top": 0, "createdAt": 1736379135, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-01-09", "dateLabelColor": "#0969da"}, "P11": {"htmlDir": "docs/post/Hands on experience on AI coding.html", "labels": ["AI", "Prompt Engineering", "Programming"], "postTitle": "Hands on experience on AI coding", "postUrl": "post/Hands%20on%20experience%20on%20AI%20coding.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/11", "commentNum": 0, "wordCount": 4482, "description": "I started using ChatGTP on writing some test code since last year, and recently I've heard people keep mentioning cursor, and used it to [make a app](https://www.reddit.com/r/cursor/comments/1h35uh6/nocode_developers_success_story_built_appstores_1/) in one hour and climbed to top 20 paid app in apple store, and he claim had no coding experience, maybe some just marketing, but still very impressive.\n\nI started to explorer the AI coding tools, like blot.new, windsurf, cursor, and finally became a windsurf paid user (tried for one month), and end up with building two tools, one Android app and one Web app. \n\nThe Android app idea comes up TikTok Refugees' requirement, they switch to RedNote but there was language barriers there b/c most users are Chinese, so I came up with an idea of [screen translation](https://play.google.com/store/apps/details?id=com.lomoware.screen_translate&pli=1), I later thought I probably should do some research first before I did this, but since this starts with experimenting on AI coding tools, so I just launch and to see whether I can finish that within one week, the prototype is ready in 2 days, and I have no experience on flutter, I choose flutter on purpose b/c I'd like to see whether it's doable with zero experience, but I do have limited experience on Android/iOS development, but not expert. As for the prototype it just can capture the screen and do the translation, show overlay text, simple. However, it's far away from a product, there are a lot of improvements need to done to make it usable:\n\n1.  support multiple languages\n2. detect scrolling so that it can translate the new content\n3. support rotation\n4. background color auto detection so that we can pick the overlay color\n5. different modes (auto translation/manual translation/original text)\n\nAnother tool I built with AI is https://insightly.top, an tool to 'Uncover Deep Insights from App Reviews', again, the tech stack is not something I familiar with, but I do have some basic knowledge on the frontend html+css+javascript. And similar time frame, around 1.5 week to go to production.\n\nI'm 100% that people with zero coding experience will not able to do this, some of those bugs are really tricky and I don't think AI can fix, and in the beginning when I use this tool, I thought it as reliable FSD, but it turns out to be a level2 auto pilot. I need to understand the code structure, the logic of how it works, and jump directly into some bug fixes, do the code review, but it DOES help improve the efficiency, and I don't think I can do the same thing within the same time frame. Some people mention AI coding are senior SDE now, I don't agree, it's not some equivalent, it's still a tool, without soul or creativity, still need human by its side to guide it, even though it can write code faster, and may have more knowledge than us. So basically those AI coding tools are just LLM + workflow integration, you just pay some convenient fees by saving the context switch effort.\n\nThere are some prompts that helps to use the coding tools:\n\n- https://github.com/PatrickJS/awesome-cursorrules\n- https://cursor.directory/\n- https://cursorlist.com/\n\nThey are helpful, but not enough, what I found that is sometimes we need play with multiple roles when working the project, you need to work on architect, frontend, backend, test, devops, and need to switch between different roles, and those prompts are just assuming you work on project with specific tech stack and that is not the future, in the future, AI will play different roles, either we have different agents with  different roles to work together, or a full stack 10x AI agent can handle everything. The missing part is how to adapt the whole engineering process to AI era. \n\n> Software engineering is the process of designing, building, testing, and maintaining software applications. It's a branch of computer science that uses engineering principles and programming languages to create software solutions. \nWhat do software engineers do? \n\n> -   Apply engineering principles to design software solutions\n> -   Use programming languages to build software\n> -   Understand algorithms, databases, and system design\n> -   Solve problems\n> -   Work in teams\n\nAnyway, I'm still exploring, and I won't go back to coding IDE without AI support, just like I won't go back to drive a car without [openpliot](https://github.com/commaai/openpilot), even though it's not perfect, it does helps, and it's the future.\u3002", "top": 0, "createdAt": 1739236957, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-02-11", "dateLabelColor": "#0969da"}, "P12": {"htmlDir": "docs/post/MTLS Setup with Hitch and Squid.html", "labels": ["Linux", "Open Source"], "postTitle": "MTLS Setup with Hitch and Squid", "postUrl": "post/MTLS%20Setup%20with%20Hitch%20and%20Squid.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/12", "commentNum": 0, "wordCount": 9360, "description": "# Install Squid\n\nUse Linux machine to install squid\n```\nsudo apt update\nsudo apt install squid\n```\n\nConfig squid, change '/etc/squid/squid.conf' and allow all traffic\n\n```\n# And finally deny all other access to this proxy\n#http_access deny all\nhttp_access allow all\n```\n\nRestart squid:\n\n```\nsudo systemctl restart squid\n```\n\nVerify squid works (10.231.249.222 is the ip address of the machine):\n\n```\ncurl -x http://10.231.249.222:3128 https://myip.ipip.net\n```\n\n# Install Hitch\n\nThen install Hitch, old version may not support MTLS, so just grab the latest one:\n\n```\nwget https://hitch-tls.org/source/hitch-1.8.0.tar.gz\ntar zxvf hitch-1.8.0.tar.gz\nsudo apt-get install libev-dev libssl-dev automake python3-docutils flex bison pkg-config make -y\ncd hitch-1.8.0\n./configure;make;sudo make install\n```\n\nCreate Hitch Config '/etc/hitch/hitch.conf' as below:\n\n```\nfrontend = {\n    host = '*'\n    port = '8443'\n}\nbackend = '[127.0.0.1]:3128'    # 6086 is the default Varnish PROXY port.\nworkers = 4                     # number of CPU cores\n\ndaemon = off\n\n# We strongly recommend you create a separate non-privileged hitch\n# user and group\nuser = 'hitch'\ngroup = 'hitch'\n\n# Enable to let clients negotiate HTTP/2 with ALPN. (default off)\n# alpn-protos = 'h2, http/1.1'\n\n# run Varnish as backend over PROXY; varnishd -a :80 -a localhost:6086,PROXY ..\n# write-proxy-v2 = on               # Write PROXY header\n\n# Only support strong ciphers and TLSv1.2, TLSv1.3\ntls-protos = TLSv1.2 TLSv1.3\nciphers = 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:!aNULL:!eNULL:!EXPORT:!LOW:!MD5:!DES:!3DES:!RC2:!RC4:!PSK:!kDH'\n\nsyslog = on\nlog-level = 2\n\n# The path to the combined server certificate and its private key.\npem-file = '/etc/hitch/cert/server.pem'\n```\n\nCreate user 'hitch':\nsudo useradd -r -U -s /bin/false hitch\n\n# Test TLS\n\nCreate server certificate and private key:\n\n```\nmkdir cert\ncd cert\nopenssl req -newkey rsa:2048 -nodes -keyout server.key -x509 -days 365 -out server.crt -subj '/CN=yourdomain.com' -extensions SAN -config <(cat /etc/ssl/openssl.cnf <(printf '\\n[SAN]\\nsubjectAltName=DNS:yourdomain.com,DNS:www.yourdomain.com'))\ncat server.key server.crt > server.pem\nsudo mkdir -p /etc/hitch/cert\nsudo cp server.pem /etc/hitch/cert/\n```\n\nAdd domain name in '/etc/hosts':\n\n```\n10.231.249.222 www.yourdomain.com\n```\n\nAdd the Self-Signed Certificate to the Trust Store:\n\n```\nsudo cp server.crt /usr/local/share/ca-certificates/my-self-signed.crt\nsudo update-ca-certificates\n```\n\nRun Hitch:\n\n```\n/usr/local/sbin/hitch --test --config /etc/hitch/hitch.conf # validate conf first\n/usr/local/sbin/hitch --user hitch --group hitch --config /etc/hitch/hitch.conf --daemon\n```\n\nTest with curl:\n\n```\ncurl -x https://www.yourdomain.com:8443 https://myip.ipip.net\n```\n\n# Test MTLS\n\nCreate the 'client.ext' file with the following content:\n\n```\n# client.ext\nbasicConstraints=CA:FALSE\nnsCertType=client\nnsComment='OpenSSL Generated Client Certificate'\nsubjectKeyIdentifier=hash\nauthorityKeyIdentifier=keyid,issuer:always\nkeyUsage=critical,digitalSignature,keyEncipherment\nextendedKeyUsage=clientAuth\n```\n\nCreate client certificate and private key:\n\n```\nopenssl genrsa -out ca.key 2048\nopenssl req -new -x509 -days 3650 -key ca.key -out ca.crt\n\nopenssl genrsa -out client.key 2048\nopenssl req -new -key client.key -out client.csr\nopenssl x509 -req -days 365 -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile client.ext -out client.crt\n\nsudo cp ca.crt /etc/hitch/cert/\n```\n\nNow add the following content in '/etc/hitch/hitch.conf' to enable MTLS on Hitch\n\n```\n# This forces clients to present a valid client certificate.\nclient-verify = required\n\n# Client certificate authority file used to verify client certificates for mTLS.\nclient-verify-ca = '/etc/hitch/cert/ca.crt'\n```\n\nNow run Hitch\n\n```\n/usr/local/sbin/hitch --test --config /etc/hitch/hitch.conf # validate conf first\n/usr/local/sbin/hitch --user hitch --group hitch --config /etc/hitch/hitch.conf --daemon\n```\n\nVerify MTLS setup with curl\n\n```\n      MTLS           TLS           TLS\ncurl -------> Hitch ------> Squid -----> Dst\n```\n\n```\ncurl --cacert ca.crt --proxy-cert client.crt --proxy-key client.key -x https://www.yourdomain.com:8443  https://myip.ipip.net\n```\n\n> [!NOTE]\n> Should be noticed that currently only tls3 can be used in this case, https://github.com/curl/curl/issues/12286, so it's hard to verify from wireshark, since all certificate exchanges are encrypted after the TLS handshake. But we have Hitch configured to require MTLS, so if it succeeds, it should be using MTLS.\n\n# Enable Proxy Protocol\n\nThe PROXY protocol was designed to help carry connection information from the source requesting side through a proxy server to the destination, especially when network address translation (NAT) is involved. It provides a way to preserve the original end-user IP addresses and TCP/UDP port numbers when traffic is routed through proxies and load balancers. The information is typically added to the header of the request forwarded to the destination server.\n\nThere are two versions of the PROXY protocol: V1 and V2.\n- V1 is human-readable ASCII, V2 is binary.\n- V1 headers are simpler but more verbose, V2 headers are more flexible and can carry additional information using TLV (Type-Length-Value) vectors..\n- V2 can support protocols other than TCP, e.g., UDP.\nIt needs to be enabled at both Squid and Hitch.\nAdd the following in '/etc/hitch/hitch.conf':\n\n```\nproxy-proxy = on\n```\n\nChange 'http_port 3128' in '/etc/squid/squid.conf':\n\n```\nhttp_port 3128 require-proxy-header\nproxy_protocol_access allow localnet\n\n# following one is needed when testing on the same machine\nacl localnet src 127.0.0.1\n```\n\nVerify MTLS setup with curl\n\n```\ncurl --cacert ca.crt --proxy-cert client.crt --proxy-key client.key -x https://www.yourdomain.com:8443 --haproxy-protocol https://myip.ipip.net\n```\n\n# Cert and Key\n\nHere's the step-by-step process for creating a set of certificates to be used for MTLS, including the CA certificate and key, server certificate and key, and client certificate and key. Please replace 'Your CA Name', 'yourdomain.com', and 'Your Server Name' with your actual CA, domain, and server names.\n\n## Step 1: Generate the CA's Private Key\n\n```\nopenssl genrsa -out ca.key 4096\n```\n\nThis creates a new 4096-bit RSA private key for your CA. The key will be stored in ca.key.\n\n## Step 2: Generate the Self-Signed Root CA Certificate\n\n```\nopenssl req -new -x509 -days 3650 -key ca.key -out ca.crt -subj '/CN=Your CA Name'\n```\n\nThis command generates a new self-signed CA certificate based on the private key (ca.key) you created. The -days 3650 makes the certificate valid for 10 years. The certificate will be stored in ca.crt.\n\n## Step 3: Generate the Server's Private Key\n\n```\nopenssl genrsa -out server.key 2048\n```\n\nGenerate a new 2048-bit RSA private key for your server. The key will be stored in server.key.\n\n## Step 4: Create a Certificate Signing Request (CSR) for the Server\n\n```\nopenssl req -new -key server.key -out server.csr -subj '/CN=www.yourdomain.com'\n```\n\nThis creates a new CSR (server.csr) for your server certificate using the server's private key (server.key).\n\n## Step 5: Generate the Server Certificate Signed by Your CA\n\n```\nopenssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt\n```\n\nThis signs the server CSR with your CA key, resulting in a signed server certificate (server.crt) valid for 1 year.\n\n## Step 6: Generate the Client's Private Key\n\n```\nopenssl genrsa -out client.key 2048\n```\n\nGenerate a new 2048-bit RSA private key for the client. This key will be stored in client.key.\n\n## Step 7: Create a CSR for the Client\n\n```\nopenssl req -new -key client.key -out client.csr -subj '/CN=Your Client Name'\n```\n\nCreate a CSR (client.csr) for the client using the newly created client key (client.key).\n\n## Step 8: Generate the Client Certificate Signed by Your CA\n\n```\nopenssl x509 -req -days 365 -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt\n```\n\nThis signs the client CSR with your CA key, creating a signed client certificate (client.crt) valid for 1 year. This certificate is to be used for mutual TLS authentication by the client.\n\n## Step 9 (optional): Verify the Certificates\n\nTo verify the server and client certificates were signed by your CA:\n\n```\nopenssl verify -CAfile ca.crt server.crt\nopenssl verify -CAfile ca.crt client.crt\n```\n\nBoth commands should return OK if everything went well.\n\nNow you have:\n- CA certificate (ca.crt) installed and trusted on the server and clients.\n- Server certificate (server.crt) and private key (server.key) are installed on the server.\n- Client certificate (client.crt) and private key (client.key) are installed on the client.\n\nMake sure you protect your private keys (ca.key, server.key, client.key) by setting appropriate file permissions. Using these instructions, your server and client can mutually authenticate themselves over a TLS connection, with your own CA acting as the trust anchor.\nThis content is only supported in a Feishu Docs\n\nReference\nhttps://www.haproxy.org/download/1.8/doc/proxy-protocol.txt\nhttps://seriousben.com/posts/2020-02-exploring-the-proxy-protocol/\u3002", "top": 0, "createdAt": 1741733301, "style": "<style>.markdown-alert{padding:0.5rem 1rem;margin-bottom:1rem;border-left:.25em solid var(--borderColor-default,var(--color-border-default));}.markdown-alert .markdown-alert-title {display:flex;font-weight:var(--base-text-weight-medium,500);align-items:center;line-height:1;}.markdown-alert>:first-child {margin-top:0;}.markdown-alert>:last-child {margin-bottom:0;}</style><style>.markdown-alert.markdown-alert-note {border-left-color:var(--borderColor-accent-emphasis, var(--color-accent-emphasis));background-color:var(--color-accent-subtle);}.markdown-alert.markdown-alert-note .markdown-alert-title {color: var(--fgColor-accent,var(--color-accent-fg));}</style>", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-03-12", "dateLabelColor": "#0969da"}, "P13": {"htmlDir": "docs/post/Facebook Marketing API resources.html", "labels": ["AI"], "postTitle": "Facebook Marketing API resources", "postUrl": "post/Facebook%20Marketing%20API%20resources.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/13", "commentNum": 0, "wordCount": 1307, "description": "If you want to manage the Facebook Ads with API, you should check the marketing API. I'm thinking about build MCP server for facebook ads management.\n\nThe [document](https://developers.facebook.com/docs/marketing-api/) is not friendly for the new comers, especially the auth token setup, there is no screenshot, and outdated, and both the doc and the pages are messy with lots of stuffs there. Luckily I found this YouTube video '[Facebook Graph API: A Full Guide for Marketers and Non-Techies](https://www.youtube.com/watch?v=IFM3Otvb7So)' very helpful. \n\nAnd checkout [Facebook API Tutorial: Graph API, Access Token and Developer Documentation Explained](https://www.youtube.com/watch?v=to4uTxSNo6Q)\n\nOnce you watched the videos, you can check https://www.postman.com/meta/facebook-marketing-api/overview and play with it.\n\nAnd auotmation with make, check video https://www.youtube.com/watch?v=MjzUlHcuUtA,  you can download the template at https://neilstephenson.beehiiv.com/p/video-tutorial-automated-facebook-ads-creative-testing.\n\nAnother related is [ads library API](https://www.facebook.com/ads/library/api/), but it's not used to manage your Ads, it's providing your Ads stats, as you can see https://www.facebook.com/ads/library/report, and do some search at https://www.facebook.com/ads/library/.\u3002", "top": 0, "createdAt": 1743308154, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-03-30", "dateLabelColor": "#0969da"}, "P14": {"htmlDir": "docs/post/AI-Powered Due Diligence Platform.html", "labels": ["AI"], "postTitle": "AI-Powered Due Diligence Platform", "postUrl": "post/AI-Powered%20Due%20Diligence%20Platform.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/14", "commentNum": 0, "wordCount": 6466, "description": "# \ud83d\udee0\ufe0f 1. Core Product Modules\n\n## \ud83d\udce5 A. Data Ingestion Layer\n\nThis layer collects, cleans, and unifies all deal data.\n\nInputs Accepted:\n\n- Accounting software (QuickBooks, Xero, NetSuite)\n- Bank & credit card statements (via Plaid or file upload)\n- POS exports (e.g., Square, Toast, Clover)\n- Payroll providers (Gusto, ADP, Paychex)\n- Tax returns / W-2s / 1099s (PDF extraction + OCR)\n- Broker CIMs (to extract seller-provided claims)\n\n\nFeatures:\n\n- Smart mapping of chart of accounts to standard LMM schema\n- OCR + LLM-based auto-classification of scanned PDFs\n- Historical trend normalization (standardizes fiscal year vs calendar year)\n- Reconciliation engine: Ensures consistency between P&L, bank deposits, and cash on hand\n\n## \ud83e\udde0 B. LLM-Powered Analysis Engine\n\nThe heart of the platform\u2014your AI due diligence analyst.\n\nCapabilities:\n\n1. Financial Normalization & Adjustments\n\n- Owner compensation benchmarking (flag excessive salaries)\n- Detection of owner perks/addbacks (cars, travel, family on payroll)\n- One-time / non-recurring event detection (lawsuits, one-off projects)\n- Revenue recognition alignment (cash vs accrual)\n\n2. Variance & Trend Analysis\n\n- Monthly / YoY revenue and margin swings with LLM-generated commentary\n- Seasonality detection\n- Expense trend outliers (e.g., sudden jump in subcontractor costs)\n\n3. Cash Flow & Working Capital\n\n- 13-week cash flow snapshot\n- NWC normalization for deal structuring\n- Inventory/purchasing cycle risk flags\n\n4. Customer & Vendor Concentration\n\n- Top customer/vendor reliance by %\n- LLM summarizes risk from churned customers\n- Lifetime value and cohort revenue retention if CRM data is provided\n\n5. Labor & Operational Analysis\n\n- Payroll breakdown by department\n- Staff-to-revenue ratio benchmarking (e.g., techs per $1M HVAC revenue)\n- Compensation benchmarking vs BLS or industry averages\n\n\n## \ud83d\udcca C. Output: AI-Generated Due Diligence Report\n\nDeliverables:\n\n- PDF + Interactive Web Report\n- Standard sections:\n   - Executive Summary\n   - Financial Overview (Revenue, COGS, SG&A, EBITDA)\n   - Key Adjustments & Addbacks\n   - Working Capital & Cash Flow Snapshot\n   - Customer/Vendor Analysis\n   - Risk Flags Summary\n   - Red Flag Watchlist\n- Dynamic footnotes: Every metric includes LLM-generated rationale\n- Toggle view: \u201cBroker View\u201d vs \u201cBuyer View\u201d vs \u201cBank View\u201d to tailor emphasis\n\n## \ud83d\udcc8 D. Benchmarks & Valuation Module (Optional Add-On)\n\n- Compares business KPIs to industry averages (NAICS + zip code)\n- Suggests valuation range (3x\u20135x EBITDA, etc.) based on risk-adjusted metrics\n- Creates a pre-filled investment memo with comps + deal notes\n- Flagging system:\n  - \ud83d\udd34 Red: Customer concentration, cash-based revenue, no controls\n  - \ud83d\udfe0 Yellow: Addbacks >15% of EBITDA, weak gross margins\n  - \ud83d\udfe2 Green: Clean books, minimal owner involvement, recurring revenue\n\n# \ud83d\udd10 2. Security, Accuracy & Explainability\n\n## A. Explainability Layer\n\n- All LLM-generated insights are linked to source docs\n- Allows users to click on a number or sentence and see:\n  - What doc it came from\n  - The prompt that generated it\n  - LLM confidence score (0\u2013100%)\n\n## B. Reviewer Console\n\n- Human analysts or advisors can review and override any section\n- Edits logged for audit trail (critical for trust with banks or LPs)\n\n# \ud83e\udd16 3. Integrations\n\n- Accounting: QuickBooks, Xero, NetSuite\n- Banking: Plaid, MX\n- POS: Square, Clover, Shopify\n- Payroll: Gusto, ADP, Paychex\n- CRM (optional): HubSpot, Salesforce\n- Brokerage platforms: Axial, Acquire.com (for future deal flow ingestion)\n\n# \ud83c\udfaf 4. Target Use Cases\n\nUse Case | Persona | Outcome\n-- | -- | --\nPre-LOI red flag review | Searcher | Spend <$1K to screen out bad deals\nPost-LOI due diligence | PE Associate | Replace $30K QoE with faster insights\nBroker-side pre-listing | M&A Advisor | Improve CIM quality & credibility\nFinancing partner prep | SBA lender or equity co-investor | Faster underwriting with standardized KPIs\n\n\n\n# \ud83d\udccc 5. Example Scenarios\n\nHVAC Company ($3M EBITDA, cash-based)\n \u2192 System detects 30% cash revenue unreported on books, large owner addbacks, top 2 clients = 50% revenue\n\nMulti-location Med Spa ($1.2M EBITDA)\n \u2192 Flag: Owner has 2 Porsches on P&L, rent above market rate on affiliate-owned RE\n\nFranchise Restaurant ($4.5M Rev)\n \u2192 System extracts POS data, flags declining AUV and labor cost creep vs category median\n\n# Reasons for QofE \n\nThe primary reason for conducting a QofE is its direct impact on the purchase price. It evaluates 3 critical factors, helping buyers assess if the business price is reasonable.\n\n- Enterprise Value \nDetermined by taking a multiple of EBITDA or similar financial measures, and analyzing the financials to determine if that financial measures is accurately reported as well as any key factors that impact the multiple \n\n- Net Debt \nAssessment of debt like items is done to highlight any potential reductions to Enterprise Value and future cash flows\n\n- Net Working Capital\nIf less than a normal level of working capital is delivered at the closing of a transaction the buyer may be required to contribute additional capital in order to fund operations.\n\nA comprehensive analysis of the business is needed to ensure adequate working capital is delivered at closing.\n\t\nAdvises self funded searchers, traditional searchers, private equity groups, family offices, and strategic acquirers in evaluating acquisitions with transaction values up to $50 million. \n\n\n## Industries\n\nHVAC\nGovernment Contracting\nProfessional Services\nRetail\nLandscaping\nE-Commerce\nHealthcare\nManufacturing\nPlumbing\nTrucking\nRestaurants\nReal Estate/Construction\nMarketing Agencies\nElectrical Contracting\n\n\n## Is QofE different than an audit? \n\nAn audit is different from a QoE in a variety of ways. For one an audit is a regulated procedure that has specific guidelines for how it needs to be completed. A QoE is a consulting engagement that can technically be done by anyone although they are typically done by CPAs.\nAn audit is much more balance sheet focused while a QoE is more focused on the income statement. The audit is looking to confirm that the financials comply with GAAP while a QoE helps a buyer understand the financial performance of the company that can be projected into the future as well as the key operating metrics.\nOn deals where audited financials are available the audit is complementary to the QoE.\n\n[Understanding QofE](https://corporatefinanceinstitute.com/resources/valuation/quality-of-earnings-report/)\n\n\n\n\u3002", "top": 0, "createdAt": 1748231117, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-05-26", "dateLabelColor": "#0969da"}, "P15": {"htmlDir": "docs/post/Quality of Earning Report-bi-ji.html", "labels": ["Books"], "postTitle": "Quality of Earning Report\u7b14\u8bb0", "postUrl": "post/Quality%20of%20Earning%20Report-bi-ji.html", "postSourceUrl": "https://github.com/JiantaoFu/jiantaofu.github.io/issues/15", "commentNum": 0, "wordCount": 15008, "description": "# Abbr\n\nM&A - Mergers and Acquisitions\n\nGAAP - Generally Accepted Accounting Principles, \u662f\u6307\u4e00\u5957\u5728\u7f8e\u56fd\u4f7f\u7528\u7684\u4f1a\u8ba1\u539f\u5219\u3001\u6807\u51c6\u548c\u60ef\u4f8b\u3002", "top": 0, "createdAt": 1748282603, "style": "", "script": "<script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script>", "head": "", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "createdDate": "2025-05-27", "dateLabelColor": "#0969da"}}, "singeListJson": {}, "labelColorDict": {"AI": "#c5def5", "Books": "#c2e0c6", "Business": "#bfdadc", "Linux": "#f9d0c4", "Music": "#6D11DC", "Open Source": "#6D3DBF", "Programming": "#006b75", "Prompt Engineering": "#E6CF51", "System Design": "#0101FA", "WebRTC": "#fbca04"}, "displayTitle": "The Curious Path", "faviconUrl": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "ogImage": "https://www.gravatar.com/avatar/b2e648e70e0d3d2b8970e089ec487bc9?s=200&d=identicon&r=g", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://jiantaofu.github.io", "prevUrl": "disabled", "nextUrl": "disabled"}